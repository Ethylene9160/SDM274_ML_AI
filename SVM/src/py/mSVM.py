import numpy as np
import cvxopt

class SVM:
    def __init__(self, kernel='linear', C=1.0, degree=3, gamma=None, coef0=0):
        self.kernel = kernel
        self.C = C
        self.degree = degree
        self.gamma = gamma
        self.coef0 = coef0

    def fit(self, X, y):
        ################### Solve the quadratic programming problem ###################
        ################### Generated by other people, not me ###################
        m, n = X.shape

        # Initialize gamma based on n_features if it is None
        if self.gamma is None:
            self.gamma = 1 / n

        # Calculate the kernel matrix
        K = self._compute_kernel(X)

        # Construct the matrices for the quadratic programming problem
        P = cvxopt.matrix(np.outer(y, y) * K)
        q = cvxopt.matrix(-np.ones((m, 1)))
        G = cvxopt.matrix(np.vstack((-np.eye(m), np.eye(m))))
        h = cvxopt.matrix(np.hstack((np.zeros(m), np.ones(m) * self.C)))
        A = cvxopt.matrix(y, (1, m), 'd')
        b = cvxopt.matrix(0.0)

        # Solve the quadratic programming problem
        solution = cvxopt.solvers.qp(P, q, G, h, A, b)
        alphas = np.ravel(solution['x'])

        # Support vectors have non zero lagrange multipliers
        sv = alphas > 1e-5
        ind = np.arange(len(alphas))[sv]
        self.alphas = alphas[sv]
        self.sv = X[sv]
        self.sv_y = y[sv]

        ############################ End of the quadratic programming problem ############################
        # Calculate b
        # We use the formular:
        # b = 1/n_sv * sum_i (y_i - sum_j (alpha_j * y_j * K(x_i, x_j)))(see in SVM_Main.ipynb)
        self.b = 0
        for n in range(len(self.alphas)):
            self.b += self.sv_y[n]
            self.b -= np.sum(self.alphas * self.sv_y * K[ind[n], sv])
        self.b /= len(self.alphas)

        # Calculate weights
        # We use the formular:
        # w = sum_i (alpha_i * y_i * x_i)(see in SVM_Main.ipynb)
        # if self.kernel == 'linear':
        #     self.w = np.zeros(n)
        #     for n in range(len(self.alphas)):
        #         self.w += self.alphas[n] * self.sv_y[n] * self.sv[n]
        # else:
        #     self.w = None
        self.w = None

    # Different Kernals
    def _compute_kernel(self, X):
        if self.kernel == 'linear':
            print('pass1')
            return np.dot(X, X.T)
        elif self.kernel == 'polynomial':
            return (np.dot(X, X.T) + self.coef0) ** self.degree
        elif self.kernel == 'rbf':
            X_sq = np.sum(X ** 2, axis=1)
            K = X_sq[:, None] + X_sq[None, :] - 2 * np.dot(X, X.T)
            return np.exp(-self.gamma * K)
        elif self.kernel == 'sigmoid':
            return np.tanh(self.gamma * np.dot(X, X.T) + self.coef0)
        else:
            raise ValueError("Unknown kernel type")

    def predict(self, X):
        if self.w is not None:
            return np.sign(np.dot(X, self.w) + self.b)
        else:
            y_predict = np.zeros(X.shape[0])
            for i in range(X.shape[0]):
                prediction = 0
                # We use the formular:
                # f(x) = sum_i (alpha_i * y_i * K(x_i, x))(see in SVM_Main.ipynb)
                for alpha, sv_y, sv in zip(self.alphas, self.sv_y, self.sv):
                    if self.kernel == 'polynomial':
                        kernel_value = (np.dot(sv, X[i]) + self.coef0) ** self.degree
                    elif self.kernel == 'rbf':
                        kernel_value = np.exp(-self.gamma * np.linalg.norm(sv - X[i]) ** 2)
                    elif self.kernel == 'sigmoid':
                        kernel_value = np.tanh(self.gamma * np.dot(sv, X[i]) + self.coef0)
                    elif self.kernel == 'linear':
                        kernel_value = np.dot(sv, X[i])
                    prediction += alpha * sv_y * kernel_value
                prediction += self.b
                # change to 1 or -1.
                if prediction > 0:
                    y_predict[i] = 1
                else:
                    y_predict[i] = -1
            return y_predict
